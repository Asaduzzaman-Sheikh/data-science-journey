# ğŸ“š Complete Correlation Guide

## From Fundamentals to Advanced Applications

---

# Table of Contents

1. [What is Correlation?](#chapter-1-what-is-correlation)
2. [The Three Types of Correlation](#chapter-2-the-three-types-of-correlation)
3. [Visualizing Correlation with Scatter Plots](#chapter-3-visualizing-correlation-with-scatter-plots)
4. [The Pearson Correlation Coefficient (r)](#chapter-4-the-pearson-correlation-coefficient-r)
5. [Coefficient of Determination (rÂ²)](#chapter-5-coefficient-of-determination-rÂ²)
6. [Spearman Rank Correlation](#chapter-6-spearman-rank-correlation)
7. [Kendall Tau Correlation](#chapter-7-kendall-tau-correlation)
8. [Point-Biserial Correlation](#chapter-8-point-biserial-correlation)
9. [Correlation Matrix & Heatmaps](#chapter-9-correlation-matrix--heatmaps)
10. [Correlation vs Causation (Deep Dive)](#chapter-10-correlation-vs-causation-deep-dive)
11. [Limitations of Correlation Analysis](#chapter-11-limitations-of-correlation-analysis)
12. [Statistical Significance & Hypothesis Testing](#chapter-12-statistical-significance--hypothesis-testing)
13. [Real-World Applications](#chapter-13-real-world-applications)
14. [Practice Problems](#chapter-14-practice-problems)
15. [Quick Reference Cheat Sheet](#chapter-15-quick-reference-cheat-sheet)

---

# Chapter 1: What is Correlation?

## ğŸ¯ The Fundamental Question

Before diving into any math or formulas, correlation answers one simple question:

> **"When one thing changes, does the other thing tend to change too?"**

---

## ğŸ“– Definition

### Formal Definition

**Correlation** is a statistical measure that describes the **strength** and **direction** of a relationship between two variables.

### Intuitive Definition

**Correlation** is a way to measure **whether two things tend to move together**.

### Key Insight

Correlation is about **patterns** and **tendencies**, not absolute rules.

- We say "tend to" â€” not "always" or "definitely"
- A tall person _might_ weigh less than a short person
- A student who studies a lot _might_ still fail
- But **on average**, **in general** â€” we see a pattern

---

## ğŸ­ Real-Life Examples

| Example                                   | Relationship                                         |
| ----------------------------------------- | ---------------------------------------------------- |
| Study hours â†” Exam scores                 | Students who study more _tend to_ score higher       |
| Temperature â†” Ice cream sales             | As temperature rises, ice cream sales _tend to_ rise |
| Exercise â†” Body weight                    | People who exercise more _tend to_ weigh less        |
| Age â†” Shoe size (adults)                  | No relationship â€” shoe size doesn't change with age  |
| Class participation â†” Teacher recognition | More participation â†’ Teacher notices you more        |

---

## âš ï¸ Critical Rule: Correlation â‰  Causation

This is one of the most important concepts in statistics!

### What It Means

> **Correlation only tells us that two things move together â€” it says NOTHING about whether one causes the other.**

### Classic Example

**Ice cream sales** and **drowning deaths** are correlated:

- Both increase in summer
- Both decrease in winter

Does ice cream cause drowning? **Absolutely not!**

A **third factor** (hot weather) causes both:

- Hot weather â†’ People buy ice cream
- Hot weather â†’ People go swimming â†’ More drownings

### Remember

- Correlation: "These two things move together"
- Causation: "This thing CAUSES that thing"
- **Moving together â‰  Causing each other**

---

## ğŸ”‘ Key Takeaways

1. **Correlation measures tendency** â€” how two variables move in relation to each other
2. **It's about patterns**, not guarantees
3. **Correlation â‰  Causation** â€” this cannot be emphasized enough
4. **Knowing correlation helps prediction** â€” if two things are correlated, knowing one helps us guess the other

---

## ğŸ“š Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CORRELATION                        â”‚
â”‚                                                     â”‚
â”‚  â€¢ Measures if two things MOVE TOGETHER             â”‚
â”‚  â€¢ About TENDENCIES, not absolute rules             â”‚
â”‚  â€¢ Helps with PREDICTION                            â”‚
â”‚  â€¢ Does NOT prove CAUSATION                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Chapter 2: The Three Types of Correlation

## ğŸ¯ Overview

Now that we know _what_ correlation is, let's learn the **three types** based on **direction**.

---

## ğŸ“ˆ Type 1: Positive Correlation

### Definition

> When one variable goes **UP**, the other **ALSO goes UP**.

### Analogy

Think of two friends walking in the **same direction**.

### Visual Pattern

```
    â†—ï¸ Points go from bottom-left to top-right
   â€¢  â€¢
  â€¢  â€¢
 â€¢  â€¢
â€¢ â€¢
```

### Real-Life Examples

| Variable 1         | Variable 2          | Pattern                      |
| ------------------ | ------------------- | ---------------------------- |
| Height             | Weight              | Taller â†’ Heavier             |
| Study hours        | Exam score          | More study â†’ Higher scores   |
| Experience (years) | Salary              | More experience â†’ Higher pay |
| Temperature        | AC electricity bill | Hotter â†’ Higher bills        |
| Age (children)     | Vocabulary size     | Older â†’ More words           |

---

## ğŸ“‰ Type 2: Negative Correlation

### Definition

> When one variable goes **UP**, the other goes **DOWN**.

### Analogy

Think of a **seesaw** â€” when one side goes up, the other comes down.

### Visual Pattern

```
â€¢ â€¢    â†˜ï¸ Points go from top-left to bottom-right
 â€¢  â€¢
  â€¢  â€¢
   â€¢  â€¢
```

### Real-Life Examples

| Variable 1     | Variable 2    | Pattern                        |
| -------------- | ------------- | ------------------------------ |
| Exercise       | Body fat      | More exercise â†’ Less fat       |
| Speed          | Travel time   | Faster â†’ Less time             |
| Price          | Quantity sold | Higher price â†’ Fewer sales     |
| Absences       | Exam score    | More absences â†’ Lower scores   |
| Study hours    | Failure rate  | More study â†’ Less failure      |
| Overtime hours | Free time     | More overtime â†’ Less free time |

---

## â¡ï¸ Type 3: Zero (No) Correlation

### Definition

> **No relationship**. One variable tells you **NOTHING** about the other.

### Analogy

Think of two strangers walking **randomly** â€” their movements have no connection.

### Visual Pattern

```
  â€¢    â€¢      Points scattered randomly - no pattern
â€¢     â€¢   â€¢
   â€¢    â€¢
 â€¢   â€¢     â€¢
```

### Real-Life Examples

| Variable 1     | Variable 2   | Why No Relationship               |
| -------------- | ------------ | --------------------------------- |
| Shoe size      | Intelligence | Physical trait vs mental ability  |
| Hair color     | Salary       | Appearance vs job performance     |
| Birthday month | Height       | Birth timing vs genetics          |
| Phone number   | Exam score   | Random number vs academic ability |
| Phone battery  | Stress level | Device state vs emotional state   |

---

## ğŸ¨ Complete Summary Table

| Type             | Direction | Visual | One Goes Up...  | Example             |
| ---------------- | --------- | ------ | --------------- | ------------------- |
| **Positive (+)** | Same      | â†—ï¸     | Other goes UP   | Height â†” Weight     |
| **Negative (-)** | Opposite  | â†˜ï¸     | Other goes DOWN | Speed â†” Travel time |
| **Zero (0)**     | None      | ğŸ”€     | Other is RANDOM | Shoe size â†” IQ      |

---

## âš ï¸ Common Misconception

### âŒ Wrong Thinking

> "Negative correlation means a BAD or WEAK relationship"

### âœ… Correct Understanding

> "Negative correlation is just as STRONG as positive â€” it only means OPPOSITE directions"

### The Truth About Strength

- **Sign (+ or -)** â†’ Tells you the **DIRECTION**
- **Magnitude (the number)** â†’ Tells you the **STRENGTH**

| Correlation  | Direction           | Strength           |
| ------------ | ------------------- | ------------------ |
| +0.9         | Positive            | Very Strong        |
| -0.9         | Negative            | Very Strong        |
| +0.3         | Positive            | Weak               |
| -0.3         | Negative            | Weak               |
| +0.8 vs -0.8 | Opposite directions | **EQUAL strength** |

---

## ğŸ”‘ Key Takeaways

1. **Positive correlation**: Both variables move in the SAME direction
2. **Negative correlation**: Variables move in OPPOSITE directions
3. **Zero correlation**: No relationship at all
4. **Sign = Direction**, **Magnitude = Strength**
5. **-0.9 is stronger than +0.3** (compare absolute values!)

---

# Chapter 3: Visualizing Correlation with Scatter Plots

## ğŸ¯ What is a Scatter Plot?

A **scatter plot** shows each data point as a dot on a graph:

- **X-axis** = One variable
- **Y-axis** = Other variable
- **Each dot** = One observation (one data pair)

---

## ğŸ“Š How to Read Scatter Plots

### Positive Correlation (â†—ï¸)

```
Points go from BOTTOM-LEFT to TOP-RIGHT

       â€¢  â€¢
      â€¢  â€¢
     â€¢  â€¢
    â€¢  â€¢
   â€¢ â€¢
```

- As x increases, y increases
- Example: Study hours vs Exam score

### Negative Correlation (â†˜ï¸)

```
Points go from TOP-LEFT to BOTTOM-RIGHT

â€¢ â€¢
 â€¢  â€¢
  â€¢  â€¢
   â€¢  â€¢
    â€¢  â€¢
```

- As x increases, y decreases
- Example: Speed vs Travel time

### Zero Correlation (ğŸ”€)

```
Points scattered RANDOMLY - no pattern

  â€¢    â€¢
â€¢     â€¢   â€¢
   â€¢    â€¢
 â€¢   â€¢     â€¢
```

- No relationship between x and y
- Example: Shoe size vs IQ

---

## ğŸ” Assessing Correlation Strength from Scatter Plots

### Perfect Correlation (|r| = 1.0)

- All points fall **exactly on a straight line**
- Extremely rare in real data

### Strong Correlation (|r| > 0.7)

- Points cluster **tightly** around a line
- Clear pattern visible

### Moderate Correlation (0.4 < |r| < 0.7)

- Points show a **general trend** but with spread
- Pattern visible but not tight

### Weak Correlation (|r| < 0.4)

- Points show **slight tendency**
- Lots of scatter, pattern hard to see

### Zero Correlation (r â‰ˆ 0)

- Points appear **random**
- No visible pattern

---

## ğŸ Python Code for Scatter Plots

### Basic Setup

```python
import numpy as np
import matplotlib.pyplot as plt
```

### Creating Sample Data

#### Positive Correlation Data

```python
np.random.seed(42)  # For reproducibility
n = 50  # Number of data points

# X variable: random values
x = np.random.uniform(1, 10, n)

# Y variable: increases with x (plus noise)
y = 50 + 5 * x + np.random.normal(0, 5, n)
```

#### Negative Correlation Data

```python
# X variable
x = np.random.uniform(30, 100, n)

# Y variable: decreases with x (note the MINUS sign)
y = 200 - 1.5 * x + np.random.normal(0, 10, n)
```

#### Zero Correlation Data

```python
# X and Y are completely independent
x = np.random.uniform(6, 12, n)
y = np.random.uniform(85, 130, n)  # Doesn't use x at all!
```

### Creating the Scatter Plot

```python
plt.figure(figsize=(8, 6))
plt.scatter(x, y, color='green', alpha=0.7, s=80)
plt.xlabel('X Variable')
plt.ylabel('Y Variable')
plt.title('Scatter Plot Title')
plt.show()
```

---

## ğŸ¨ Scatter Plot Parameters

| Parameter        | What It Does               | Example                     |
| ---------------- | -------------------------- | --------------------------- |
| `figsize=(8, 6)` | Width and height in inches | 8 wide, 6 tall              |
| `color='green'`  | Dot color                  | 'red', 'blue', '#FF5733'    |
| `alpha=0.7`      | Transparency (0-1)         | 0.7 = slightly see-through  |
| `s=80`           | Dot size                   | Larger number = bigger dots |

---

## ğŸ”‘ Key Takeaways

1. **Scatter plots visualize relationships** between two variables
2. **Direction of points** shows correlation type:
   - â†—ï¸ Bottom-left to top-right = Positive
   - â†˜ï¸ Top-left to bottom-right = Negative
   - ğŸ”€ Random scatter = Zero
3. **Tightness of clustering** shows correlation strength
4. **Not all points follow the pattern** â€” that's normal (it's about tendency)
5. **Noise makes data realistic** â€” perfect lines don't exist in real world

---

# Chapter 4: The Pearson Correlation Coefficient (r)

## ğŸ¯ Why Do We Need a Number?

Looking at scatter plots, you can **see** correlation visually. But you can't:

- Measure it precisely
- Compare strengths objectively
- Communicate it clearly

**Solution:** The correlation coefficient (**r**) â€” a single number that captures both direction and strength.

---

## ğŸ“ The Range of r

```
   -1 â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•0â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’ +1
    â”‚              â”‚              â”‚
 Perfect         No           Perfect
 Negative    Correlation     Positive
```

### Interpretation Guide

| r value      | Strength        | Direction |
| ------------ | --------------- | --------- |
| -1.0         | Perfect         | Negative  |
| -0.9 to -0.7 | Strong          | Negative  |
| -0.7 to -0.4 | Moderate        | Negative  |
| -0.4 to -0.1 | Weak            | Negative  |
| -0.1 to +0.1 | None/Negligible | â€”         |
| +0.1 to +0.4 | Weak            | Positive  |
| +0.4 to +0.7 | Moderate        | Positive  |
| +0.7 to +0.9 | Strong          | Positive  |
| +1.0         | Perfect         | Positive  |

### Two Components of r

```
r = -0.75
    â”‚  â”‚
    â”‚  â””â”€â”€ Magnitude (0.75) â†’ STRENGTH
    â”‚
    â””â”€â”€â”€â”€â”€ Sign (-) â†’ DIRECTION
```

---

## ğŸ“ The Two Formulas

### Version 1: Conceptual/Deviation Formula

$$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \cdot \sum(y_i - \bar{y})^2}}$$

**Best for:** Understanding WHY correlation works

### Version 2: Computational/Raw Score Formula

$$r = \frac{n(\sum xy) - (\sum x)(\sum y)}{\sqrt{[n\sum x^2 - (\sum x)^2][n\sum y^2 - (\sum y)^2]}}$$

**Best for:** Exam calculations (fewer steps!)

---

## ğŸ“– Symbol Meanings

### Version 1 Symbols

| Symbol            | Meaning                               |
| ----------------- | ------------------------------------- |
| $x_i$             | Each individual x value               |
| $y_i$             | Each individual y value               |
| $\bar{x}$         | Mean (average) of all x values        |
| $\bar{y}$         | Mean (average) of all y values        |
| $(x_i - \bar{x})$ | Deviation: how far x is from its mean |

### Version 2 Symbols

| Symbol       | How to Calculate                     |
| ------------ | ------------------------------------ |
| $n$          | Count of data pairs                  |
| $\sum x$     | Add all x values                     |
| $\sum y$     | Add all y values                     |
| $\sum xy$    | Multiply each pair, then add all     |
| $\sum x^2$   | Square each x, then add all          |
| $(\sum x)^2$ | Add all x first, then square the sum |

---

## âš ï¸ CRITICAL: Common Mistake

### $\sum x^2$ vs $(\sum x)^2$ â€” NOT THE SAME!

| Notation     | Steps                  | Example (x = 2, 3, 5) |
| ------------ | ---------------------- | --------------------- |
| $\sum x^2$   | Square FIRST, then add | $4 + 9 + 25 = 38$     |
| $(\sum x)^2$ | Add FIRST, then square | $(10)^2 = 100$        |

**This is the #1 mistake on exams!**

---

## ğŸ“Š Worked Example

**Data:** Study Hours (x) vs Exam Score (y)

| Student | x   | y   |
| ------- | --- | --- |
| A       | 2   | 65  |
| B       | 3   | 70  |
| C       | 5   | 80  |
| D       | 7   | 85  |
| E       | 8   | 90  |

### Step 1: Build Table

| Student | x      | y       | xy       | xÂ²      | yÂ²        |
| ------- | ------ | ------- | -------- | ------- | --------- |
| A       | 2      | 65      | 130      | 4       | 4225      |
| B       | 3      | 70      | 210      | 9       | 4900      |
| C       | 5      | 80      | 400      | 25      | 6400      |
| D       | 7      | 85      | 595      | 49      | 7225      |
| E       | 8      | 90      | 720      | 64      | 8100      |
| **Sum** | **25** | **390** | **2055** | **151** | **30850** |

### Step 2: Identify Values

- n = 5
- Î£x = 25
- Î£y = 390
- Î£xy = 2055
- Î£xÂ² = 151
- Î£yÂ² = 30850

### Step 3: Calculate Numerator

$$n(\sum xy) - (\sum x)(\sum y) = 5(2055) - (25)(390) = 10275 - 9750 = 525$$

### Step 4: Calculate Denominator

First bracket:
$$n\sum x^2 - (\sum x)^2 = 5(151) - (25)^2 = 755 - 625 = 130$$

Second bracket:
$$n\sum y^2 - (\sum y)^2 = 5(30850) - (390)^2 = 154250 - 152100 = 2150$$

Square root:
$$\sqrt{130 \times 2150} = \sqrt{279500} = 528.77$$

### Step 5: Final Answer

$$r = \frac{525}{528.77} = 0.993$$

### Step 6: Interpretation

- **Sign:** Positive (+) â†’ Same direction
- **Magnitude:** 0.993 â†’ Very strong
- **Conclusion:** Very strong positive correlation

---

## ğŸ Python Code for Correlation

### Using Built-in Functions (Recommended)

```python
import numpy as np
import pandas as pd
from scipy.stats import pearsonr

# NumPy
r = np.corrcoef(x, y)[0, 1]

# Pandas
r = df['column1'].corr(df['column2'])

# SciPy (also gives p-value)
r, p_value = pearsonr(x, y)
```

### Manual Implementation (Version 2)

```python
n = len(x)
sum_x = np.sum(x)
sum_y = np.sum(y)
sum_xy = np.sum(x * y)
sum_x2 = np.sum(x**2)
sum_y2 = np.sum(y**2)

numerator = n * sum_xy - sum_x * sum_y
denominator = np.sqrt((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))

r = numerator / denominator
```

---

## ğŸ”‘ Key Takeaways

1. **r ranges from -1 to +1**
2. **Sign = Direction**, **Magnitude = Strength**
3. **Two formulas exist** â€” same result, different purposes
4. **Version 2 is easier for exams** â€” make a table, sum columns
5. **Watch out for $\sum x^2$ vs $(\sum x)^2$** â€” #1 mistake!
6. **In practice, use built-in functions** â€” but understand the concept

---

# Chapter 5: Coefficient of Determination (rÂ²)

## ğŸ¯ What is rÂ²?

The **coefficient of determination (rÂ²)** tells us:

> **What percentage of the variation in Y is explained by X?**

---

## ğŸ“ Formula & Calculation

### Formula

$$r^2 = r \times r$$

Simply **square the correlation coefficient**.

### Example

If r = 0.8:
$$r^2 = 0.8 \times 0.8 = 0.64 = 64\%$$

This means: **64% of the variation in Y is explained by X**

---

## ğŸ“Š Interpretation

| r   | rÂ²         | Interpretation                       |
| --- | ---------- | ------------------------------------ |
| 0.9 | 0.81 (81%) | X explains 81% of variation in Y     |
| 0.7 | 0.49 (49%) | X explains 49% of variation in Y     |
| 0.5 | 0.25 (25%) | X explains 25% of variation in Y     |
| 0.3 | 0.09 (9%)  | X explains only 9% of variation in Y |

### Key Insight

- Even a "moderate" correlation of 0.5 only explains 25% of the variation!
- This is why we shouldn't overinterpret moderate correlations

---

## ğŸ¨ Visual Understanding

```
Total Variation in Y
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   Explained     â”‚  â”‚ Unexplained â”‚  â”‚
â”‚   â”‚    by X         â”‚  â”‚  (random or â”‚  â”‚
â”‚   â”‚    (rÂ²)         â”‚  â”‚   other     â”‚  â”‚
â”‚   â”‚                 â”‚  â”‚   factors)  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ Important Notes

1. **rÂ² is always positive** (because you're squaring)
2. **rÂ² is always between 0 and 1** (or 0% to 100%)
3. **rÂ² loses direction information** â€” you need r to know positive vs negative
4. **Higher rÂ² means better prediction**

---

## ğŸ“ When to Use r vs rÂ²

| Use    | When                                             |
| ------ | ------------------------------------------------ |
| **r**  | You want to know direction AND strength          |
| **rÂ²** | You want to know how much variation is explained |

---

## ğŸ Python Code

```python
# Method 1: Square the correlation
r = np.corrcoef(x, y)[0, 1]
r_squared = r ** 2

# Method 2: Using sklearn (for regression context)
from sklearn.metrics import r2_score
r_squared = r2_score(y_actual, y_predicted)
```

---

# Chapter 6: Spearman Rank Correlation

## ğŸ¯ When Pearson Isn't Enough

Pearson correlation assumes:

1. Linear relationship
2. Continuous data
3. No extreme outliers

When these assumptions fail, use **Spearman rank correlation**.

---

## ğŸ“– Definition

**Spearman correlation** measures the relationship between the **ranks** of data, not the actual values.

---

## ğŸ“ Formula

$$\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}$$

Where:

- $d_i$ = Difference between ranks of x and y for each pair
- $n$ = Number of data pairs

---

## ğŸ“Š Step-by-Step Calculation

### Original Data

| Observation | X   | Y   |
| ----------- | --- | --- |
| 1           | 10  | 50  |
| 2           | 20  | 40  |
| 3           | 15  | 55  |
| 4           | 25  | 30  |
| 5           | 30  | 35  |

### Step 1: Rank Each Variable (1 = smallest)

| Observation | X   | Rank_X | Y   | Rank_Y |
| ----------- | --- | ------ | --- | ------ |
| 1           | 10  | 1      | 50  | 4      |
| 2           | 20  | 3      | 40  | 3      |
| 3           | 15  | 2      | 55  | 5      |
| 4           | 25  | 4      | 30  | 1      |
| 5           | 30  | 5      | 35  | 2      |

### Step 2: Calculate Differences

| Observation | Rank_X | Rank_Y | d   | dÂ²     |
| ----------- | ------ | ------ | --- | ------ |
| 1           | 1      | 4      | -3  | 9      |
| 2           | 3      | 3      | 0   | 0      |
| 3           | 2      | 5      | -3  | 9      |
| 4           | 4      | 1      | 3   | 9      |
| 5           | 5      | 2      | 3   | 9      |
| **Sum**     |        |        |     | **36** |

### Step 3: Apply Formula

$$\rho = 1 - \frac{6 \times 36}{5(25-1)} = 1 - \frac{216}{120} = 1 - 1.8 = -0.8$$

**Interpretation:** Strong negative correlation in ranks

---

## ğŸ¨ Pearson vs Spearman Comparison

| Aspect     | Pearson (r)         | Spearman (Ï)               |
| ---------- | ------------------- | -------------------------- |
| Measures   | Linear relationship | Monotonic relationship     |
| Data type  | Continuous          | Ordinal or continuous      |
| Outliers   | Very sensitive      | Robust                     |
| Non-linear | Misses it           | Captures it                |
| Assumption | Normal distribution | No distribution assumption |

---

## ğŸ“ˆ When to Use Which?

### Use Pearson (r) when:

- Relationship appears linear
- Data is continuous
- No extreme outliers
- Normal distribution

### Use Spearman (Ï) when:

- Ordinal data (rankings, Likert scales)
- Non-linear but monotonic relationship
- Outliers present
- Distribution is skewed

---

## ğŸ Python Code

```python
from scipy.stats import spearmanr

# Calculate Spearman correlation
rho, p_value = spearmanr(x, y)

print(f"Spearman Ï = {rho:.4f}")
print(f"p-value = {p_value:.4f}")
```

---

# Chapter 7: Kendall Tau Correlation

## ğŸ¯ When to Use Kendall

- **Small sample sizes** (more robust than Spearman for n < 30)
- **Ordinal data** with many ties
- Measures **pair agreement** (concordance)
- When you need a more **conservative** estimate

---

## ğŸ“– The Concept

Kendall Tau measures how often pairs of observations are in the **same order** for both variables.

- **Concordant pair:** When xâ‚ > xâ‚‚ AND yâ‚ > yâ‚‚ (or both less) â€” pairs agree
- **Discordant pair:** When xâ‚ > xâ‚‚ BUT yâ‚ < yâ‚‚ (or vice versa) â€” pairs disagree

---

## ğŸ“ Formula

$$\tau = \frac{(\text{Concordant pairs}) - (\text{Discordant pairs})}{\frac{n(n-1)}{2}}$$

Or equivalently:
$$\tau = \frac{C - D}{\binom{n}{2}}$$

Where:

- C = Number of concordant pairs
- D = Number of discordant pairs
- n = Number of observations

---

## ğŸ“Š Worked Example

**Data:**

| Person | X   | Y   |
| ------ | --- | --- |
| A      | 1   | 2   |
| B      | 2   | 3   |
| C      | 3   | 1   |

**Step 1: List all pairs**

- (A, B): X: 1 < 2 âœ“, Y: 2 < 3 âœ“ â†’ **Concordant** (both increase)
- (A, C): X: 1 < 3 âœ“, Y: 2 > 1 âœ— â†’ **Discordant** (X increases, Y decreases)
- (B, C): X: 2 < 3 âœ“, Y: 3 > 1 âœ— â†’ **Discordant** (X increases, Y decreases)

**Step 2: Count**

- Concordant (C) = 1
- Discordant (D) = 2
- Total pairs = n(n-1)/2 = 3(2)/2 = 3

**Step 3: Calculate Ï„**
$$\tau = \frac{1 - 2}{3} = \frac{-1}{3} = -0.33$$

**Interpretation:** Weak negative correlation

---

## ğŸ¨ Spearman vs Kendall Comparison

| Aspect             | Spearman (Ï)            | Kendall (Ï„)                       |
| ------------------ | ----------------------- | --------------------------------- |
| **Measures**       | Distance between ranks  | Pair agreement                    |
| **Best for**       | Larger samples (n > 30) | Smaller samples (n < 30)          |
| **Sensitivity**    | Less robust to ties     | More robust to ties               |
| **Interpretation** | Similar to Pearson      | Probability of agreement          |
| **Magnitude**      | Usually higher          | Usually lower (more conservative) |

---

## ğŸ Python Code

```python
from scipy.stats import kendalltau

# Calculate Kendall correlation
tau, p_value = kendalltau(x, y)

print(f"Kendall Ï„ = {tau:.4f}")
print(f"p-value = {p_value:.4f}")
```

---

# Chapter 8: Point-Biserial Correlation

## ğŸ¯ When to Use Point-Biserial

Use when you have:

- One **binary/dichotomous** variable (0/1, Yes/No, Male/Female)
- One **continuous** variable (scores, amounts, measurements)

---

## ğŸ“– Definition

**Point-biserial correlation** measures the relationship between a binary categorical variable and a continuous variable.

---

## ğŸ­ Real-Life Examples

| Binary Variable    | Continuous Variable | Question Answered                       |
| ------------------ | ------------------- | --------------------------------------- |
| Gender (M/F)       | Test Score          | Do males and females score differently? |
| Treatment (Yes/No) | Recovery Time       | Does treatment affect recovery?         |
| Pass/Fail          | Study Hours         | Is study time related to passing?       |
| Smoker (Yes/No)    | Lung Capacity       | Does smoking affect lung capacity?      |
| Employed (Yes/No)  | Happiness Score     | Is employment related to happiness?     |

---

## ğŸ“ Formula

$$r_{pb} = \frac{M_1 - M_0}{s_n}\sqrt{\frac{n_1 n_0}{n^2}}$$

Where:

- $M_1$ = Mean of continuous variable for group 1 (binary = 1)
- $M_0$ = Mean of continuous variable for group 0 (binary = 0)
- $s_n$ = Standard deviation of all continuous values
- $n_1$ = Sample size of group 1
- $n_0$ = Sample size of group 0
- $n$ = Total sample size

---

## ğŸ“Š Worked Example

**Data:** Does gender affect test scores?

| Person | Gender (1=M, 0=F) | Test Score |
| ------ | ----------------- | ---------- |
| A      | 1                 | 85         |
| B      | 1                 | 90         |
| C      | 0                 | 75         |
| D      | 0                 | 70         |
| E      | 1                 | 88         |
| F      | 0                 | 72         |

**Step 1: Calculate means by group**

- $M_1$ (Males) = (85 + 90 + 88) / 3 = 87.67
- $M_0$ (Females) = (75 + 70 + 72) / 3 = 72.33

**Step 2: Calculate overall standard deviation**

- All scores: 85, 90, 75, 70, 88, 72
- Mean = 80
- $s_n$ â‰ˆ 8.37

**Step 3: Count groups**

- $n_1$ = 3 (males)
- $n_0$ = 3 (females)
- $n$ = 6

**Step 4: Apply formula**
$$r_{pb} = \frac{87.67 - 72.33}{8.37}\sqrt{\frac{3 \times 3}{36}} = \frac{15.34}{8.37} \times 0.5 = 0.92$$

**Interpretation:** Strong positive correlation â€” males tend to score higher in this sample.

---

## ğŸ Python Code

```python
from scipy.stats import pointbiserialr

# binary = array of 0s and 1s
# continuous = array of continuous values

binary = [1, 1, 0, 0, 1, 0]
continuous = [85, 90, 75, 70, 88, 72]

r_pb, p_value = pointbiserialr(binary, continuous)

print(f"Point-Biserial r = {r_pb:.4f}")
print(f"p-value = {p_value:.4f}")
```

---

## âš ï¸ Important Notes

1. **Order of binary coding matters for sign** â€” r will be positive if group 1 has higher values
2. **Mathematically equivalent to Pearson** when applied to binary data
3. **Effect size interpretation** is the same as Pearson (0.1 = small, 0.3 = medium, 0.5 = large)
4. **Assumptions:** Continuous variable should be approximately normal within each group

---

# Chapter 9: Correlation Matrix & Heatmaps

## ğŸ¯ What is a Correlation Matrix?

When you have **many variables**, calculating pairwise correlations one by one is tedious. A **correlation matrix** shows ALL correlations at once.

---

## ğŸ“Š Structure

For variables A, B, C:

|       | A      | B      | C      |
| ----- | ------ | ------ | ------ |
| **A** | 1.00   | r(A,B) | r(A,C) |
| **B** | r(B,A) | 1.00   | r(B,C) |
| **C** | r(C,A) | r(C,B) | 1.00   |

### Properties

1. **Diagonal is always 1** (variable correlated with itself)
2. **Symmetric** â€” r(A,B) = r(B,A)
3. **Values range from -1 to +1**

---

## ğŸ Python Code

### Creating Correlation Matrix

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
data = {
    'hours_studied': [2, 4, 6, 8, 10, 12, 14, 16],
    'exam_score': [55, 60, 65, 70, 75, 80, 85, 90],
    'sleep_hours': [7, 6.5, 6, 5.5, 5, 4.5, 4, 3.5],
    'coffee_cups': [1, 1, 2, 2, 3, 3, 4, 4]
}
df = pd.DataFrame(data)

# Calculate correlation matrix
corr_matrix = df.corr()
print(corr_matrix)
```

### Creating a Heatmap

```python
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix,
            annot=True,          # Show numbers in cells
            cmap='coolwarm',     # Color scheme
            center=0,            # Center color at 0
            vmin=-1, vmax=1,     # Fixed range
            fmt='.2f')           # 2 decimal places
plt.title('Correlation Matrix Heatmap')
plt.tight_layout()
plt.show()
```

---

## ğŸ¨ Reading Heatmaps

### Color Interpretation

- **Red/Dark** = Strong positive correlation
- **Blue/Cold** = Strong negative correlation
- **White/Light** = No correlation

### What to Look For

1. **Strong correlations** (dark colors) â€” potential relationships
2. **Multicollinearity** â€” when predictors correlate with each other
3. **Target correlations** â€” how features relate to your target variable

---

## âš ï¸ Common Pitfalls

1. **Diagonal is always 1** â€” don't get excited about it!
2. **Correlation â‰  importance** â€” low correlation doesn't mean useless
3. **Non-linear relationships** â€” might show low r but still be meaningful
4. **Sample size matters** â€” small n can produce misleading correlations

---

# Chapter 8: Correlation vs Causation (Deep Dive)

## ğŸ¯ The Most Important Statistical Concept

> **Correlation does NOT imply causation.**

This is often stated but rarely understood deeply. Let's fix that.

---

## ğŸ“– Why Correlation Doesn't Mean Causation

### 1. Third Variable Problem (Confounding)

```
                    C (Third Variable)
                   â•± â•²
                  â†“   â†“
                 A     B
                  â†˜   â†™
              (Appear correlated)
```

**Example:**

- A = Ice cream sales
- B = Drowning deaths
- C = Hot weather (the REAL cause)

### 2. Reverse Causation

Maybe Y causes X, not X causes Y!

**Example:**

- Correlation: Happy people earn more money
- Assumption: Money causes happiness
- Reality: Maybe happiness causes earning (confidence, energy, etc.)

### 3. Coincidence

With enough variables, some will correlate by pure chance.

**Example:**

- Nicolas Cage movies correlate with swimming pool drownings
- Clearly no real connection!

---

## ğŸ“Š Real Examples of Spurious Correlations

| Correlation                                 | Why It's Meaningless  |
| ------------------------------------------- | --------------------- |
| Cheese consumption â†” Bedsheet death         | Pure coincidence      |
| Age of Miss America â†” Steam burns           | Random correlation    |
| Letters in spelling bee word â†” Spider bites | No logical connection |

---

## âœ… How to Establish Causation

To prove A causes B, you need:

### 1. Correlation

A and B must be correlated (necessary but not sufficient)

### 2. Temporal Precedence

A must happen BEFORE B

### 3. Elimination of Alternatives

Rule out other possible causes (confounders)

### 4. Mechanism

Explain HOW A causes B (optional but helpful)

### 5. Experimental Evidence

Random controlled trials (gold standard)

---

## ğŸ”‘ Key Takeaways

1. **Always ask:** "What else could explain this correlation?"
2. **Look for:** Third variables, reverse causation, coincidence
3. **To prove causation:** Need experiments, not just observation
4. **In doubt:** Say "associated with" not "causes"

---

# Chapter 9: Limitations of Correlation Analysis

## ğŸ¯ What Correlation CAN'T Tell You

Understanding limitations makes you a better analyst.

---

## âš ï¸ Limitation 1: Non-Linear Relationships

### The Problem

Pearson correlation only detects **LINEAR** relationships.

### Example

```
Y
â”‚    â€¢    â€¢
â”‚   â€¢      â€¢
â”‚  â€¢        â€¢
â”‚ â€¢          â€¢
â”‚â€¢            â€¢
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
```

This U-shaped curve has r â‰ˆ 0, but there's clearly a relationship!

### Solution

- Visualize data BEFORE calculating r
- Use Spearman for monotonic non-linear
- Consider polynomial or other models

---

## âš ï¸ Limitation 2: Outlier Sensitivity

### The Problem

A single extreme value can dramatically change r.

### Example

Without outlier: r = 0.1 (weak)
With outlier: r = 0.9 (strong)

### Solution

- Always plot your data
- Consider removing outliers or using Spearman
- Report with and without outliers

---

## âš ï¸ Limitation 3: Restriction of Range

### The Problem

If your sample doesn't capture full range, correlation is underestimated.

### Example

Studying IQ vs Grades only among gifted students (all high IQ) â†’ Low correlation

### Solution

- Ensure sample represents full population range
- Be aware when interpreting restricted samples

---

## âš ï¸ Limitation 4: Assumes Linear, Normal Data

### Pearson Assumptions

1. Variables are continuous
2. Linearly related
3. Approximately normal
4. Homoscedastic (equal variance)

### When Violated

- Use Spearman rank correlation
- Use transformations
- Use non-parametric methods

---

## âš ï¸ Limitation 5: Doesn't Show Causation

Already covered in Chapter 8, but worth repeating:

> **Correlation â‰  Causation**

---

## âš ï¸ Limitation 6: Sample Size Matters

### Small Samples

- Correlations are unstable
- Wide confidence intervals
- Can be statistically insignificant even if strong

### General Rule

- n < 30: Be very cautious
- n = 30-100: Moderate confidence
- n > 100: Good reliability

---

# Chapter 10: Statistical Significance & Hypothesis Testing

## ğŸ¯ Is This Correlation Real or Just Luck?

A correlation in your sample might not exist in the population. We need to test this!

---

## ğŸ“– The Null Hypothesis

**Hâ‚€:** There is NO correlation in the population (Ï = 0)
**Hâ‚:** There IS a correlation in the population (Ï â‰  0)

---

## ğŸ“ The t-test for Correlation

### Formula

$$t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}}$$

### Degrees of Freedom

$$df = n - 2$$

### Decision

- If |t| > critical value â†’ Reject Hâ‚€ â†’ Correlation is significant
- If |t| â‰¤ critical value â†’ Fail to reject Hâ‚€ â†’ Correlation might be due to chance

---

## ğŸ“Š P-Value Interpretation

| p-value   | Interpretation     |
| --------- | ------------------ |
| p < 0.001 | Highly significant |
| p < 0.01  | Very significant   |
| p < 0.05  | Significant        |
| p â‰¥ 0.05  | Not significant    |

---

## âš ï¸ Important Warnings

### Statistical vs Practical Significance

| r    | n      | p-value | Statistically Significant? | Practically Meaningful? |
| ---- | ------ | ------- | -------------------------- | ----------------------- |
| 0.05 | 10,000 | < 0.001 | Yes                        | NO â€” trivial effect     |
| 0.80 | 10     | 0.06    | No                         | YES â€” strong effect     |

**Key insight:** With large n, even tiny correlations become "significant"

---

## ğŸ Python Code

```python
from scipy.stats import pearsonr

# Calculate correlation and p-value
r, p_value = pearsonr(x, y)

print(f"Correlation: r = {r:.4f}")
print(f"P-value: p = {p_value:.4f}")

if p_value < 0.05:
    print("Statistically significant at Î± = 0.05")
else:
    print("Not statistically significant at Î± = 0.05")
```

---

# Chapter 11: Real-World Applications

## ğŸ¢ Industry Applications

### Finance & Economics

- Stock price correlations for portfolio diversification
- Interest rates vs inflation
- GDP vs unemployment

### Healthcare & Medicine

- Risk factors vs disease outcomes
- Drug dosage vs response
- Biomarker relationships

### Marketing & Sales

- Advertising spend vs revenue
- Customer satisfaction vs retention
- Price vs demand

### Social Sciences

- Education level vs income
- Social media use vs mental health
- Parenting styles vs child outcomes

---

## ğŸ“Š Portfolio Diversification Example

### Goal

Invest in assets that are NOT correlated (or negatively correlated)

### Correlation Matrix for Assets

|             | Stocks | Bonds | Gold | Real Estate |
| ----------- | ------ | ----- | ---- | ----------- |
| Stocks      | 1.00   | -0.30 | 0.10 | 0.50        |
| Bonds       | -0.30  | 1.00  | 0.20 | 0.15        |
| Gold        | 0.10   | 0.20  | 1.00 | 0.05        |
| Real Estate | 0.50   | 0.15  | 0.05 | 1.00        |

### Insight

- Stocks & Bonds: Negative correlation â†’ Good diversification
- Stocks & Real Estate: Moderate positive â†’ Some overlap in risk

---

## ğŸ©º Medical Research Example

### Study: Exercise and Cholesterol

```python
# Hypothetical data
exercise_hours = [0, 1, 2, 3, 4, 5, 6, 7]
cholesterol = [240, 230, 220, 210, 195, 180, 170, 165]

r, p = pearsonr(exercise_hours, cholesterol)
# r = -0.99, p < 0.001
```

**Interpretation:**

- Strong negative correlation
- More exercise associated with lower cholesterol
- But: Correlation alone doesn't prove exercise CAUSES lower cholesterol

---

# Chapter 12: Practice Problems

## ğŸ“ Problem Set

### Problem 1: Calculate r by Hand

**Data:**

| x   | y   |
| --- | --- |
| 1   | 2   |
| 2   | 4   |
| 3   | 5   |
| 4   | 4   |
| 5   | 5   |

Calculate the Pearson correlation coefficient.

<details>
<summary>Click for Solution</summary>

1. n = 5
2. Î£x = 15, Î£y = 20
3. Î£xy = 50
4. Î£xÂ² = 55, Î£yÂ² = 86
5. Numerator = 5(50) - (15)(20) = 250 - 300 = -50
6. Denominator = âˆš[(5Ã—55 - 225)(5Ã—86 - 400)] = âˆš[(50)(30)] = âˆš1500 = 38.73
7. **r = -50/38.73 = -1.29** â†’ Error! Recalculate...

Actually: Î£xy = 1Ã—2 + 2Ã—4 + 3Ã—5 + 4Ã—4 + 5Ã—5 = 2 + 8 + 15 + 16 + 25 = 66

Numerator = 5(66) - 15(20) = 330 - 300 = 30
**r = 30/38.73 â‰ˆ 0.77**

</details>

---

### Problem 2: Interpret Correlations

For each r value, describe the strength and direction:

1. r = 0.92
2. r = -0.45
3. r = 0.08
4. r = -0.88

<details>
<summary>Click for Solution</summary>

1. r = 0.92 â†’ **Strong positive** correlation
2. r = -0.45 â†’ **Moderate negative** correlation
3. r = 0.08 â†’ **Negligible/no** correlation
4. r = -0.88 â†’ **Strong negative** correlation

</details>

---

### Problem 3: r vs rÂ²

If r = 0.6, what percentage of variance in Y is explained by X?

<details>
<summary>Click for Solution</summary>

rÂ² = 0.6Â² = 0.36 = **36%**

36% of the variance in Y is explained by X.

</details>

---

### Problem 4: Correlation vs Causation

A study finds a strong positive correlation between shoe size and reading ability in children. Does bigger feet help with reading?

<details>
<summary>Click for Solution</summary>

**No!** This is a classic confounding variable problem.

The third variable is **age**:

- Older children â†’ Bigger feet
- Older children â†’ Better reading

Age causes both, so shoe size and reading appear correlated even though one doesn't cause the other.

</details>

---

# Chapter 13: Quick Reference Cheat Sheet

## ğŸ“‹ Correlation Coefficient (r)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PEARSON CORRELATION COEFFICIENT (r)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RANGE: -1 â‰¤ r â‰¤ +1                                         â”‚
â”‚                                                             â”‚
â”‚  SIGN: + positive, - negative                               â”‚
â”‚  MAGNITUDE: closer to 1 = stronger                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FORMULA (for exams):                                       â”‚
â”‚                                                             â”‚
â”‚         n(Î£xy) - (Î£x)(Î£y)                                   â”‚
â”‚    r = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚        âˆš[nÎ£xÂ² - (Î£x)Â²][nÎ£yÂ² - (Î£y)Â²]                        â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEPS:                                                     â”‚
â”‚  1. Make table: x, y, xy, xÂ², yÂ²                            â”‚
â”‚  2. Sum each column                                         â”‚
â”‚  3. Plug into formula                                       â”‚
â”‚  4. Interpret: sign + magnitude                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš ï¸ REMEMBER: Î£xÂ² â‰  (Î£x)Â²                                   |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Interpretation Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          INTERPRETING r                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  |r| = 1.0       Perfect                     â”‚
â”‚  |r| = 0.7-0.9   Strong                      â”‚
â”‚  |r| = 0.4-0.7   Moderate                    â”‚
â”‚  |r| = 0.1-0.4   Weak                        â”‚
â”‚  |r| < 0.1       None/Negligible             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Python Quick Reference

```python
# Pearson correlation
import numpy as np
from scipy.stats import pearsonr, spearmanr

r = np.corrcoef(x, y)[0, 1]           # Just r
r, p = pearsonr(x, y)                  # r and p-value

# Spearman correlation
rho, p = spearmanr(x, y)

# Correlation matrix
import pandas as pd
corr_matrix = df.corr()

# Heatmap
import seaborn as sns
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```

---

## ğŸ“‹ Key Formulas

| Formula                                                                                        | Purpose              |
| ---------------------------------------------------------------------------------------------- | -------------------- |
| $r = \frac{n\sum xy - \sum x \sum y}{\sqrt{[n\sum x^2 - (\sum x)^2][n\sum y^2 - (\sum y)^2]}}$ | Pearson r            |
| $r^2$                                                                                          | % variance explained |
| $\rho = 1 - \frac{6\sum d^2}{n(n^2-1)}$                                                        | Spearman Ï           |
| $t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}}$                                                         | Significance test    |

---

## ğŸ“‹ Critical Reminders

1. âš ï¸ **Correlation â‰  Causation**
2. âš ï¸ **Always visualize first**
3. âš ï¸ **Check for non-linearity**
4. âš ï¸ **Watch out for outliers**
5. âš ï¸ **Consider sample size**
6. âš ï¸ **Î£xÂ² â‰  (Î£x)Â²**

---

## ğŸ“‹ When to Use Which Method

| Situation                        | Use                |
| -------------------------------- | ------------------ |
| Linear relationship, normal data | Pearson (r)        |
| Ordinal data                     | Spearman (Ï)       |
| Non-linear but monotonic         | Spearman (Ï)       |
| Outliers present                 | Spearman (Ï)       |
| Multiple variables               | Correlation matrix |
| Prediction testing               | rÂ²                 |

---

_Last updated: January 2026_

---
